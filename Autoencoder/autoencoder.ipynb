{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66deb508-390b-4f98-841a-b1c25cfeae8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Concatenate, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from BahdanauAttention import AttentionLayer\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2bab190d-e939-40cd-b871-ecce3cd3b729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class s2s_model:\n",
    "    def __init__(self, max_encoder_len, max_decoder_len, num_encoder_vocab, num_decoder_vocab):\n",
    "        self.latent_dim = 64\n",
    "        self.embedding_dim = 64\n",
    "        self.max_encoder_len = max_encoder_len\n",
    "        self.max_decoder_len = max_decoder_len\n",
    "        self.num_encoder_vocab = num_encoder_vocab\n",
    "        self.num_decoder_vocab = num_decoder_vocab\n",
    "        \n",
    "        self.build_encoder()\n",
    "        self.build_decoder()\n",
    "        \n",
    "        self.training_model = Model([self.encoder_inputs, self.decoder_inputs], self.decoder_outputs)\n",
    "        \n",
    "    def build_encoder(self):\n",
    "        self.encoder_inputs = Input(shape=(self.max_encoder_len, ))\n",
    "        self.encoder_embed = Embedding(self.num_encoder_vocab, self.embedding_dim, trainable=True)(self.encoder_inputs)\n",
    "        self.encoder_LSTM1 = LSTM(self.latent_dim, return_sequences=True, return_state=True, dropout = 0.4, recurrent_dropout = 0.3)\n",
    "        self.encoder_output1, self.state_h1, self.state_c1 = self.encoder_LSTM1(self.encoder_embed)\n",
    "\n",
    "        self.encoder_LSTM2 = LSTM(self.latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.3)\n",
    "        self.encoder_output2, self.state_h2, self.state_c2 = self.encoder_LSTM2(self.encoder_output1) # encoder LSTMs feed into each other\n",
    "\n",
    "        self.encoder_LSTM3 = LSTM(self.latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.3)\n",
    "        self.encoder_output, self.state_h, self.state_c = self.encoder_LSTM3(self.encoder_output2) # final outputs and states to pass to decoder LSTM\n",
    "        \n",
    "    def build_decoder(self):\n",
    "        self.decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "        # define layer architecture, then match to inputs\n",
    "        self.decoder_embed_layer = Embedding(self.num_decoder_vocab, self.embedding_dim, trainable=True)\n",
    "        self.decoder_embed = self.decoder_embed_layer(self.decoder_inputs)\n",
    "\n",
    "        # decoder LSTM layer\n",
    "        self.decoder_LSTM = LSTM(self.latent_dim, return_sequences=True, return_state= True, dropout=0.4, recurrent_dropout=0.2)\n",
    "        self.decoder_outputs, self.decoder_fwd_state, self.decoder_back_state = self.decoder_LSTM(self.decoder_embed, initial_state=[self.state_h, self.state_c])\n",
    "\n",
    "        # dense layer (output layer)\n",
    "        # keras.layers.TimeDistributed layer considers temporal dimension\n",
    "        # Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.\n",
    "        self.decoder_dense = TimeDistributed(Dense(self.num_decoder_vocab, activation='softmax'))\n",
    "        self.decoder_outputs = self.decoder_dense(self.decoder_outputs)\n",
    "        \n",
    "    def compile(self):\n",
    "        self.training_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics = ['acc'])\n",
    "        \n",
    "    def fit(self, x_tr, y_tr_in, y_tr_out, x_test, y_test_in, y_test_out, ep, batch_size):\n",
    "        tb = TensorBoard(log_dir=\"logs/\")\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "        ck = ModelCheckpoint(filepath='data/autoencoder_best_weights.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        Callbacks = [es, ck]\n",
    "        self.training_model.fit([x_tr,y_tr_in], y_tr_out, epochs = ep, callbacks=Callbacks, batch_size = batch_size, validation_data=(([x_test,y_test_in]), y_test_out))\n",
    "    \n",
    "    def build_inference_model(self):\n",
    "        self.inference_encoder_model = Model(inputs= self.encoder_inputs, outputs=[self.encoder_output, self.state_h, self.state_c])\n",
    "\n",
    "        # decoder setup\n",
    "        self.decoder_state_input_h = Input(shape=(self.latent_dim,))\n",
    "        self.decoder_state_input_c = Input(shape=(self.latent_dim,))\n",
    "        self.decoder_hidden_state_input = Input(shape=(self.max_encoder_len, self.latent_dim))\n",
    "\n",
    "        self.decoder_embed_i = self.decoder_embed_layer(self.decoder_inputs)\n",
    "\n",
    "        self.decoder_output_i, self.state_h_i, self.state_c_i = self.decoder_LSTM(self.decoder_embed_i, initial_state = [self.decoder_state_input_h, self.decoder_state_input_c])\n",
    "\n",
    "        self.decoder_output_i = self.decoder_dense(self.decoder_output_i)\n",
    "\n",
    "        # final decoder inference model\n",
    "        self.inference_decoder_model = Model([self.decoder_inputs] + [self.decoder_hidden_state_input, self.decoder_state_input_h, self.decoder_state_input_c], [self.decoder_output_i] + [self.state_h_i, self.state_c_i])\n",
    "        \n",
    "    def decode_sequence(self, input_seq, i2o, o2i):\n",
    "        e_out,e_h, e_c = self.inference_encoder_model.predict(input_seq, verbose = 0)\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = o2i['<']\n",
    "\n",
    "        stop_condition = False\n",
    "        decoded_sentence = []\n",
    "\n",
    "        while not stop_condition:\n",
    "            (output_tokens, h, c) = self.inference_decoder_model.predict([target_seq] + [e_out, e_h, e_c], verbose = 0)\n",
    "\n",
    "            # Sample a token\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_token = i2o[sampled_token_index]   \n",
    "\n",
    "            if sampled_token != '>':\n",
    "                decoded_sentence += [sampled_token]\n",
    "\n",
    "            # Exit condition: either hit max length or find the stop word.\n",
    "            if (sampled_token == '>') or (len(decoded_sentence) >= self.max_decoder_len):\n",
    "                stop_condition = True\n",
    "\n",
    "            # Update the target sequence (of length 1)\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "            # Update internal states\n",
    "            (e_h, e_c) = (h, c)\n",
    "        return decoded_sentence\n",
    "    def word2seq(self, a2i, input_word):\n",
    "        final_seq = []\n",
    "        for c in input_word:\n",
    "            final_seq += [a2i[c]]\n",
    "        final_seq = pad_sequences([final_seq], maxlen=self.max_encoder_len, padding='post')[0]\n",
    "        return final_seq\n",
    "    \n",
    "    def translate(self, input_word, a2i, i2o, o2i):\n",
    "        seq = self.word2seq(a2i, input_word).reshape(1, self.max_encoder_len)\n",
    "        return self.decode_sequence(seq, i2o, o2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10759480-0d31-4030-9167-0c6db0602cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_syl(word, indexes):\n",
    "        index_list = np.where(np.array(indexes) == 2)[0]\n",
    "        word_array = [*word]\n",
    "        for i in range(0, len(index_list)):\n",
    "            word_array.insert(index_list[i] + i + 1, '-')\n",
    "        return ''.join(word_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82928d55-cc5a-46a3-be62-b3c1bc8b67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab english words\n",
    "# grab one hot encoding of syllable boundaries\n",
    "# shuffle indexes of one hots per word\n",
    "# X = randomized indexes inserted into word\n",
    "# true Y = true indexes inserted into word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9462c32-cd03-40ea-80d2-cdcc30fac04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ox/x_tr_ortho.pkl', 'rb') as file:\n",
    "    x_tr_ortho = pickle.load(file)\n",
    "with open('data/ox/x_val_ortho.pkl', 'rb') as file:\n",
    "    x_val_ortho = pickle.load(file)\n",
    "with open('data/ox/e2i_ortho.pkl', 'rb') as file:\n",
    "    e2i_ortho = pickle.load(file)\n",
    "    i2e_ortho = {v: k for k, v in e2i_ortho.items()}\n",
    "with open('data/ox/y_tr.pkl', 'rb') as file:\n",
    "    y_tr = pickle.load(file)\n",
    "with open('data/ox/y_val.pkl', 'rb') as file:\n",
    "    y_val = pickle.load(file)\n",
    "    \n",
    "pure_english_train = [''.join([i2e_ortho[c] for c in word if c != 0]) for word in x_tr_ortho]\n",
    "pure_english_val = [''.join([i2e_ortho[c] for c in word if c != 0]) for word in x_val_ortho]\n",
    "true_indexes_tr = y_tr\n",
    "true_indexes_val = y_val\n",
    "\n",
    "syl_train_y = ['<' + insert_syl(pure_english_train[i], true_indexes_tr[i]) + '>' for i in range(0, len(pure_english_train))]\n",
    "syl_val_y = ['<' + insert_syl(pure_english_val[i], true_indexes_val[i]) + '>' for i in range(0, len(pure_english_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "487b7b5d-2335-472d-817b-0d8cd8e525e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating noisy data\n",
    "\n",
    "syl_train_x = ['<' + insert_syl(pure_english_train[i], shuffle_indexes(true_indexes_tr[i])) + '>' for i in range(0, len(pure_english_train))]\n",
    "syl_val_x = ['<' + insert_syl(pure_english_val[i], shuffle_indexes(true_indexes_val[i])) + '>' for i in range(0, len(pure_english_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f0be51f-d718-41ef-a71c-8c7371168d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding to 16\n",
    "\n",
    "def shuffle_indexes(indexes):\n",
    "    non_pad = [c for c in indexes if c != 0]\n",
    "    random.shuffle(non_pad)\n",
    "    repad = pad_sequences([non_pad], maxlen = 16, padding = 'post', value=0)[0]\n",
    "    return repad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "78eb40ba-cbe4-41ba-8870-e51fb416fdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# generating metadata and dictionaries\n",
    "# encoder data\n",
    "max_encoder_len = 0\n",
    "max_decoder_len = 0\n",
    "\n",
    "encoder_vocab = []\n",
    "decoder_vocab = []\n",
    "\n",
    "for line in syl_train_x:\n",
    "    max_encoder_len = max(max_encoder_len, len(line))\n",
    "    for c in line:\n",
    "        if c not in encoder_vocab:\n",
    "            encoder_vocab += [c]\n",
    "            \n",
    "for line in syl_val_x:\n",
    "    max_encoder_len = max(max_encoder_len, len(line))\n",
    "    for c in line:\n",
    "        if c not in encoder_vocab:\n",
    "            encoder_vocab += [c]\n",
    "            \n",
    "# decoder metadata\n",
    "\n",
    "for line in syl_train_y:\n",
    "    max_decoder_len = max(max_decoder_len, len(line))\n",
    "    for c in line:\n",
    "        if c not in decoder_vocab:\n",
    "            decoder_vocab += [c]\n",
    "\n",
    "for line in syl_val_y:\n",
    "    max_decoder_len = max(max_decoder_len, len(line))\n",
    "    for c in line:\n",
    "        if c not in decoder_vocab:\n",
    "            decoder_vocab += [c]\n",
    "            \n",
    "e2i_s2s = {c:i for i,c in enumerate(encoder_vocab)}\n",
    "d2i_s2s = {c:i for i,c in enumerate(decoder_vocab)}\n",
    "\n",
    "i2e_s2s = {v: k for k, v in e2i_s2s.items()}\n",
    "i2d_s2s = {v: k for k, v in d2i_s2s.items()}\n",
    "\n",
    "print(max_encoder_len)\n",
    "print(max_decoder_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3374f70e-8d08-4474-a8cf-9604f00dd6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding data to ints\n",
    "x_tr = [[e2i_s2s[c] for c in line] for line in syl_train_x]\n",
    "y_tr = [[d2i_s2s[c] for c in line] for line in syl_train_y]\n",
    "x_val = [[e2i_s2s[c] for c in line] for line in syl_val_x]\n",
    "y_val = [[d2i_s2s[c] for c in line] for line in syl_val_y]\n",
    "\n",
    "x_tr = pad_sequences(x_tr, maxlen = max_encoder_len, padding = 'post', value=0)\n",
    "y_tr = pad_sequences(y_tr, maxlen = max_decoder_len, padding = 'post', value=0)\n",
    "x_val = pad_sequences(x_val, maxlen = max_encoder_len, padding = 'post', value=0)\n",
    "y_val = pad_sequences(y_val, maxlen = max_decoder_len, padding = 'post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b24d6d3d-06c1-4d40-9604-575c2f0867e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/x_tr_auto.pkl', 'wb') as file:\n",
    "    pickle.dump(x_tr, file)\n",
    "with open('data/y_tr_auto.pkl', 'wb') as file:\n",
    "    pickle.dump(y_tr, file)\n",
    "with open('data/x_val_auto.pkl', 'wb') as file:\n",
    "    pickle.dump(x_val, file)\n",
    "with open('data/y_val_auto.pkl', 'wb') as file:\n",
    "    pickle.dump(y_val, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a89c91b-13a1-42e2-98c5-519f7f07d223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder_vocab))\n",
    "print(len(decoder_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "333d3a71-f327-481e-b03a-397afb36b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_encoder_len = 25\n",
    "# max_decoder_len = 25\n",
    "# num_encoder_vocab = 37\n",
    "# num_decoder_vocab = 37\n",
    "\n",
    "autoencoder = s2s_model(max_encoder_len, max_decoder_len, len(encoder_vocab), len(decoder_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c9127f14-de3b-40d5-91e3-fa886ed58534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80:20 training testing split\n",
    "split_index = int(len(x_tr) * .8)\n",
    "x_test = x_tr[split_index:]\n",
    "y_test = y_tr[split_index:]\n",
    "\n",
    "x_tr = x_tr[:split_index]\n",
    "y_tr = y_tr[:split_index]\n",
    "\n",
    "y_tr_in = np.array(y_tr)[:, :-1]\n",
    "y_tr_out = np.array(y_tr)[:, 1:]\n",
    "\n",
    "y_test_in = np.array(y_test)[:, :-1]\n",
    "y_test_out = np.array(y_test)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "20773fd9-4cd6-43a9-b6b9-ed30b8b34f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 1.5318 - acc: 0.6129\n",
      "Epoch 1: val_acc improved from -inf to 0.66823, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 49s 299ms/step - loss: 1.5318 - acc: 0.6129 - val_loss: 1.2190 - val_acc: 0.6682\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 1.1669 - acc: 0.6760\n",
      "Epoch 2: val_acc improved from 0.66823 to 0.68709, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 1.1669 - acc: 0.6760 - val_loss: 1.1083 - val_acc: 0.6871\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 1.0799 - acc: 0.6940\n",
      "Epoch 3: val_acc improved from 0.68709 to 0.70938, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 36s 291ms/step - loss: 1.0799 - acc: 0.6940 - val_loss: 1.0325 - val_acc: 0.7094\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.9934 - acc: 0.7126\n",
      "Epoch 4: val_acc improved from 0.70938 to 0.71677, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 297ms/step - loss: 0.9934 - acc: 0.7126 - val_loss: 0.9564 - val_acc: 0.7168\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.9387 - acc: 0.7226\n",
      "Epoch 5: val_acc improved from 0.71677 to 0.73284, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 292ms/step - loss: 0.9387 - acc: 0.7226 - val_loss: 0.9072 - val_acc: 0.7328\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.9090 - acc: 0.7299\n",
      "Epoch 6: val_acc improved from 0.73284 to 0.74157, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 299ms/step - loss: 0.9090 - acc: 0.7299 - val_loss: 0.8712 - val_acc: 0.7416\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.8854 - acc: 0.7361\n",
      "Epoch 7: val_acc improved from 0.74157 to 0.74833, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 38s 301ms/step - loss: 0.8854 - acc: 0.7361 - val_loss: 0.8534 - val_acc: 0.7483\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.8653 - acc: 0.7411\n",
      "Epoch 8: val_acc improved from 0.74833 to 0.75298, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 39s 312ms/step - loss: 0.8653 - acc: 0.7411 - val_loss: 0.8265 - val_acc: 0.7530\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.8480 - acc: 0.7446\n",
      "Epoch 9: val_acc improved from 0.75298 to 0.75366, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 40s 318ms/step - loss: 0.8480 - acc: 0.7446 - val_loss: 0.8198 - val_acc: 0.7537\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.8336 - acc: 0.7491\n",
      "Epoch 10: val_acc improved from 0.75366 to 0.75800, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 40s 318ms/step - loss: 0.8336 - acc: 0.7491 - val_loss: 0.8063 - val_acc: 0.7580\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.8206 - acc: 0.7523\n",
      "Epoch 11: val_acc improved from 0.75800 to 0.76230, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 38s 302ms/step - loss: 0.8206 - acc: 0.7523 - val_loss: 0.7922 - val_acc: 0.7623\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.8098 - acc: 0.7548\n",
      "Epoch 12: val_acc improved from 0.76230 to 0.76644, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 292ms/step - loss: 0.8098 - acc: 0.7548 - val_loss: 0.7730 - val_acc: 0.7664\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7993 - acc: 0.7575\n",
      "Epoch 13: val_acc did not improve from 0.76644\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 0.7993 - acc: 0.7575 - val_loss: 0.7877 - val_acc: 0.7613\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7883 - acc: 0.7598\n",
      "Epoch 14: val_acc improved from 0.76644 to 0.77506, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 36s 288ms/step - loss: 0.7883 - acc: 0.7598 - val_loss: 0.7462 - val_acc: 0.7751\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7738 - acc: 0.7643\n",
      "Epoch 15: val_acc improved from 0.77506 to 0.77746, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 0.7738 - acc: 0.7643 - val_loss: 0.7327 - val_acc: 0.7775\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7617 - acc: 0.7676\n",
      "Epoch 16: val_acc improved from 0.77746 to 0.77990, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 36s 289ms/step - loss: 0.7617 - acc: 0.7676 - val_loss: 0.7245 - val_acc: 0.7799\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7524 - acc: 0.7701\n",
      "Epoch 17: val_acc did not improve from 0.77990\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 0.7524 - acc: 0.7701 - val_loss: 0.7265 - val_acc: 0.7791\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7434 - acc: 0.7726\n",
      "Epoch 18: val_acc improved from 0.77990 to 0.78675, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 299ms/step - loss: 0.7434 - acc: 0.7726 - val_loss: 0.7036 - val_acc: 0.7868\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7350 - acc: 0.7748\n",
      "Epoch 19: val_acc did not improve from 0.78675\n",
      "125/125 [==============================] - 37s 294ms/step - loss: 0.7350 - acc: 0.7748 - val_loss: 0.7023 - val_acc: 0.7862\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7269 - acc: 0.7770\n",
      "Epoch 20: val_acc improved from 0.78675 to 0.78930, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 0.7269 - acc: 0.7770 - val_loss: 0.6901 - val_acc: 0.7893\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7199 - acc: 0.7797\n",
      "Epoch 21: val_acc improved from 0.78930 to 0.79354, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 36s 292ms/step - loss: 0.7199 - acc: 0.7797 - val_loss: 0.6781 - val_acc: 0.7935\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7119 - acc: 0.7818\n",
      "Epoch 22: val_acc did not improve from 0.79354\n",
      "125/125 [==============================] - 37s 299ms/step - loss: 0.7119 - acc: 0.7818 - val_loss: 0.6936 - val_acc: 0.7882\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7059 - acc: 0.7836\n",
      "Epoch 23: val_acc improved from 0.79354 to 0.79573, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 0.7059 - acc: 0.7836 - val_loss: 0.6715 - val_acc: 0.7957\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7000 - acc: 0.7849\n",
      "Epoch 24: val_acc improved from 0.79573 to 0.79937, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 298ms/step - loss: 0.7000 - acc: 0.7849 - val_loss: 0.6593 - val_acc: 0.7994\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6937 - acc: 0.7864\n",
      "Epoch 25: val_acc did not improve from 0.79937\n",
      "125/125 [==============================] - 36s 291ms/step - loss: 0.6937 - acc: 0.7864 - val_loss: 0.6673 - val_acc: 0.7965\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6889 - acc: 0.7878\n",
      "Epoch 26: val_acc improved from 0.79937 to 0.80098, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 297ms/step - loss: 0.6889 - acc: 0.7878 - val_loss: 0.6499 - val_acc: 0.8010\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6827 - acc: 0.7894\n",
      "Epoch 27: val_acc improved from 0.80098 to 0.80174, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 36s 289ms/step - loss: 0.6827 - acc: 0.7894 - val_loss: 0.6484 - val_acc: 0.8017\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6768 - acc: 0.7912\n",
      "Epoch 28: val_acc did not improve from 0.80174\n",
      "125/125 [==============================] - 37s 298ms/step - loss: 0.6768 - acc: 0.7912 - val_loss: 0.6505 - val_acc: 0.8007\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6705 - acc: 0.7926\n",
      "Epoch 29: val_acc improved from 0.80174 to 0.80626, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 38s 300ms/step - loss: 0.6705 - acc: 0.7926 - val_loss: 0.6318 - val_acc: 0.8063\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6658 - acc: 0.7943\n",
      "Epoch 30: val_acc did not improve from 0.80626\n",
      "125/125 [==============================] - 37s 298ms/step - loss: 0.6658 - acc: 0.7943 - val_loss: 0.6348 - val_acc: 0.8048\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6571 - acc: 0.7963\n",
      "Epoch 31: val_acc did not improve from 0.80626\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 0.6571 - acc: 0.7963 - val_loss: 0.6280 - val_acc: 0.8058\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6519 - acc: 0.7976\n",
      "Epoch 32: val_acc improved from 0.80626 to 0.80920, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 0.6519 - acc: 0.7976 - val_loss: 0.6168 - val_acc: 0.8092\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6425 - acc: 0.7999\n",
      "Epoch 33: val_acc improved from 0.80920 to 0.80949, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 296ms/step - loss: 0.6425 - acc: 0.7999 - val_loss: 0.6135 - val_acc: 0.8095\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6351 - acc: 0.8022\n",
      "Epoch 34: val_acc improved from 0.80949 to 0.81655, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 0.6351 - acc: 0.8022 - val_loss: 0.5912 - val_acc: 0.8166\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6261 - acc: 0.8046\n",
      "Epoch 35: val_acc improved from 0.81655 to 0.81769, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 298ms/step - loss: 0.6261 - acc: 0.8046 - val_loss: 0.5828 - val_acc: 0.8177\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6189 - acc: 0.8061\n",
      "Epoch 36: val_acc improved from 0.81769 to 0.81807, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 36s 292ms/step - loss: 0.6189 - acc: 0.8061 - val_loss: 0.5856 - val_acc: 0.8181\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6130 - acc: 0.8072\n",
      "Epoch 37: val_acc improved from 0.81807 to 0.82333, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 37s 299ms/step - loss: 0.6130 - acc: 0.8072 - val_loss: 0.5664 - val_acc: 0.8233\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6061 - acc: 0.8095\n",
      "Epoch 38: val_acc did not improve from 0.82333\n",
      "125/125 [==============================] - 37s 294ms/step - loss: 0.6061 - acc: 0.8095 - val_loss: 0.5893 - val_acc: 0.8157\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6011 - acc: 0.8109\n",
      "Epoch 39: val_acc improved from 0.82333 to 0.82673, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 36s 284ms/step - loss: 0.6011 - acc: 0.8109 - val_loss: 0.5530 - val_acc: 0.8267\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5940 - acc: 0.8131\n",
      "Epoch 40: val_acc did not improve from 0.82673\n",
      "125/125 [==============================] - 36s 288ms/step - loss: 0.5940 - acc: 0.8131 - val_loss: 0.5791 - val_acc: 0.8178\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5895 - acc: 0.8140\n",
      "Epoch 41: val_acc improved from 0.82673 to 0.82845, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 36s 287ms/step - loss: 0.5895 - acc: 0.8140 - val_loss: 0.5502 - val_acc: 0.8284\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5831 - acc: 0.8158\n",
      "Epoch 42: val_acc did not improve from 0.82845\n",
      "125/125 [==============================] - 35s 284ms/step - loss: 0.5831 - acc: 0.8158 - val_loss: 0.5675 - val_acc: 0.8225\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5776 - acc: 0.8176\n",
      "Epoch 43: val_acc improved from 0.82845 to 0.83321, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.5776 - acc: 0.8176 - val_loss: 0.5313 - val_acc: 0.8332\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5707 - acc: 0.8201\n",
      "Epoch 44: val_acc did not improve from 0.83321\n",
      "125/125 [==============================] - 36s 284ms/step - loss: 0.5707 - acc: 0.8201 - val_loss: 0.5404 - val_acc: 0.8305\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5651 - acc: 0.8214\n",
      "Epoch 45: val_acc improved from 0.83321 to 0.83725, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.5651 - acc: 0.8214 - val_loss: 0.5190 - val_acc: 0.8372\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5597 - acc: 0.8235\n",
      "Epoch 46: val_acc improved from 0.83725 to 0.84073, saving model to data\\autoencoder_best_weights.h5\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.5597 - acc: 0.8235 - val_loss: 0.5081 - val_acc: 0.8407\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5535 - acc: 0.8253\n",
      "Epoch 47: val_acc did not improve from 0.84073\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.5535 - acc: 0.8253 - val_loss: 0.5170 - val_acc: 0.8384\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5470 - acc: 0.8276\n",
      "Epoch 48: val_acc did not improve from 0.84073\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.5470 - acc: 0.8276 - val_loss: 0.5229 - val_acc: 0.8359\n",
      "Epoch 48: early stopping\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile()\n",
    "autoencoder.fit(x_tr, y_tr_in, y_tr_out, x_test, y_test_in, y_test_out, 50, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6cace40c-913d-4d81-a8d0-9bdefad42fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.build_inference_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "742bb8f1-83ff-4ce5-aa1c-c603a871c3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<u-n-tying>\n",
      "an-ti-a-tis\n",
      "<op-us>\n",
      "o-ran\n",
      "<incor-ru-pt-ibl-e>\n",
      "in-ter-ta-tion-al\n",
      "<co-r-oner>\n",
      "cor-e-tent\n",
      "<subtropi-c-s>\n",
      "su-tare-lous\n"
     ]
    }
   ],
   "source": [
    "for word in syl_train_x[:5]:\n",
    "    print(word)\n",
    "    print(''.join(autoencoder.translate(word, e2i_s2s, i2d_s2s, d2i_s2s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "30861634-6d12-4251-a8d0-fa4569f710a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<u-n-tying>', '<op-us>', '<incor-ru-pt-ibl-e>']\n",
      "['<un-ty-ing>', '<o-pus>', '<in-cor-rupt-i-ble>']\n"
     ]
    }
   ],
   "source": [
    "print(syl_train_x[:3])\n",
    "print(syl_train_y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c74a3-1d1e-49b0-a2e6-2be747caaa16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:laptop_sketchbook] *",
   "language": "python",
   "name": "conda-env-laptop_sketchbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
